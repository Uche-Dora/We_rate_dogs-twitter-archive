# We_rate_dogs-twitter-archive
Working on the dataset was quite challenging. I had to do a lot of browsing and question asking as this was my first attempt at pulling data from an API. My course-mates and mentors on the slack medium were quite helpful. Sites like geeks4geeks.org, and pandas.pydata.org were also very helpful.

To work on the project, tweepy and request lib. were installed. All necessary module were also imported: pandas, reuests, tweepy, os, json.

I started my wrangling process by gathering all the required data. One of the datasets ```twitter-archive-enhanced.csv``` required for the project was made available. All that was required was to download the file and read it into a data frame using the ```pd.read_csv```, which was done successfully. For the second dataset, we were provided with the URL of the page that contains information on image prediction. The file was downloaded programmatically using the ```request``` lib. The last dataset needed for the project was to be pulled from Twitter to get the number of likes and retweets. To do that successfully, I had to create a Twitter developer account which was approved and gathered my keys which included, ```the API key```, ```API secret key```, ```access token``` and ```access token secret```. These keys were used to grant authorisation using the code, ```tweepy.OAuth (API key, API secret key).set_access (access token, access token secret)``` which was assigned to a variable ‘auth’. To get access to the API and gather Twitter data, an object ‘api’ was created using the following code; ```tweepy.API (auth, wait_on_rate_limit = True)```. Tweets data are stored in json format. We were required to get these data and save them into a file ```tweet_json.txt``` after which it was to be read into a data-frame. Various methods and statements were useful in doing this some of which included the with open statement, api.get_status and json.dump methods. Running this however, returned an empty file so I resulted to using the file provided for us. I downloaded the file and read it into a data frame using the ```pd.read_csv``` specifying the ‘sep = (/t)’.

Immediately after successfully gathering all the datasets, I went ahead to access my data. I did so programmatically. I noted down all the quality and tidiness issues I noticed with the datasets.

After accessing and documenting the problems with my datasets, I went ahead to clean them. Data cleaning is the last step of data wrangling. I picked all the documented problems and worked on them. My cleaning steps were properly documented under three headings, define, code and test.
